{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla neural network does not handle sequential data. In light of this, we have RNN, which is a special type of neural network that is good in modelling sequential data. Examples of sequential data include text, audio, time series, etc. \n",
    "\n",
    "Below picture offers a great visualization of the mechanism of RNN under the hood. \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/9ecb85b81652e0442a635261463ea3ddae39512bf3d928b0a1f5b8df86ee4ca0/68747470733a2f2f6769746875622e636f6d2f446565704c6561726e696e674454552f30323435362d646565702d6c6561726e696e672d776974682d5079546f7263682f626c6f622f6d61737465722f7374617469635f66696c65732f726e6e2d756e666f6c642e706e673f7261773d31\" width=\"800\">\n",
    "\n",
    "\n",
    "RNN is effectively a loop, where RNN unit processes one unit of the input sequence at a time and then passes the result to the next one. As results are being propagated down the loop, latter RNN units will have memory about the previous units of the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are different strategies to parameter initialization which would speed up the training, e.g. Xavier, however in this example we only use the good O' random initialization method\n",
    "def init_params(hidden_size, vocab_size):\n",
    "    U = np.random.randn(hidden_size, vocab_size) * 0.1 # weights matrix applied on input\n",
    "    V = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "    W = np.random.randn(vocab_size, hidden_size) * 0.1\n",
    "    bh = np.zeros((hidden_size, 1)) # hidden state bias\n",
    "    by = np.zeros((vocab_size, 1)) # output bias\n",
    "    return U, V, W, bh, by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(x, derivative=False): # squish the value between [0,1]\n",
    "    if not derivative:\n",
    "        return 1/(1+np.exp(-x))\n",
    "    else:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    \n",
    "    \n",
    "def tanh(x, derivative=False): # squish the value between [-1,1]\n",
    "    if not derivative:\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    else:\n",
    "        return 1 - tanh(x)**2\n",
    "    \n",
    "    \n",
    "def softmax(x): # convert numbers into probabilities\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "# the input data, ie. the sequential data, is in the form of an array of vectors\n",
    "# each vector is passed to the RNN one by one\n",
    "# in each loop, there are two outputs, the output & the hidden state\n",
    "\n",
    "def forward_propagation(x, params, hidden_size):\n",
    "    # unpack params\n",
    "    U, V, W, bh, by = params\n",
    "    \n",
    "    # init outputs, hidden_states, h_prev\n",
    "    outputs, hidden_states = [], []\n",
    "    h_prev = np.zeros((hidden_size,1))\n",
    "    for i in x:\n",
    "        \n",
    "        # update current hidden state with new inputs\n",
    "        h_curr = tanh(np.dot(U, i) + np.dot(V, h_prev) + bh)\n",
    "        \n",
    "        # make prediction based on the current hidden states, which combines current input and previous hidden states\n",
    "        y_hat = softmax(np.dot(W, h_curr) + by)\n",
    "        \n",
    "        # save the results\n",
    "        hidden_states.append(h_curr)\n",
    "        outputs.append(y_hat)\n",
    "\n",
    "        # update h_prev for next iteration\n",
    "        h_prev = h_curr\n",
    "\n",
    "    return outputs, hidden_states\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_hat, y):\n",
    "    epsilon = 1e-12 # avoid log(0)\n",
    "    return -np.mean(np.log(y_hat + epsilon) * y)\n",
    "\n",
    "\n",
    "def backward_propagation(inputs, targets, outputs, hidden_states, params, learning_rate=0.01):\n",
    "    # unpack parameters\n",
    "    U, V, W, bh, by = params\n",
    "\n",
    "    # initialize gradients as zeros\n",
    "    dU, dV, dW, dbh, dby = np.zeros_like(U), np.zeros_like(\n",
    "        V), np.zeros_like(W), np.zeros_like(bh), np.zeros_like(by)\n",
    "\n",
    "    # initialize loss - the total loss will be the sum of loss of each timestep of the input sequence\n",
    "    loss = 0\n",
    "    \n",
    "    # initialize hidden state derivatives\n",
    "    # need to keep track of hidden state derivatives of each timestep since the dh of a given timestep is the sum of current dh & the dh of the next timestep\n",
    "    dh_next = np.zeros_like(hidden_states[0])\n",
    "    \n",
    "    # compute the gradients from back to start\n",
    "    for t in reversed(range(len(outputs))):\n",
    "        # t -> time-step\n",
    "        \n",
    "        # compute the loss and add to total loss\n",
    "        loss += cross_entropy_loss(y_hat=outputs[t], y=targets[t])\n",
    "        \n",
    "        # derivative of loss wrt y_hat\n",
    "        dy = outputs[t].copy()\n",
    "        # derivation proof: https://cs231n.github.io/neural-networks-case-study/#grad\n",
    "        # dy[np.argmax(targets[t])] -= 1 -- from Andrej Karpathy's example (https://gist.github.com/karpathy/d4dee566867f8291f086)\n",
    "        dy = dy - targets[t] # tho less efficient, this line has the same output as the above, but I think this is more intuitive and readable\n",
    "        # dy shape -> (vocab_size, 1)\n",
    "        \n",
    "        # derivative of loss wrt W\n",
    "        # dW shape -> (vocab_size, hidden_size) -> (vocab_size, 1) * (hidden_size,1).T \n",
    "        dW = np.dot(dy, hidden_states[t].T)\n",
    "        dby += dy\n",
    "        \n",
    "        # derivative of loss wrt hidden state\n",
    "        # dh shape -> (hidden_size, 1)\n",
    "        dh = np.dot(W.T, dy) + dh_next\n",
    "        \n",
    "        # derivative of h_raw wrt U -> dh * dh/d_tanh\n",
    "        # h_raw -> raw hidden state vector before passing to the tanh activation function\n",
    "        # dh shape -> (hidden_size, 1)\n",
    "        # d_tanh shape -> (hidden_size, 1)\n",
    "        # element-wise multiplication instead of np.dot to maintain the shape of (hidden_size, 1)\n",
    "        dh_raw = dh * tanh(hidden_states[t], derivative=True)\n",
    "        \n",
    "        # derivative of loss wrt U -> dh_raw * dh_raw/dU\n",
    "        # input shape -> (vocab_size, 1)\n",
    "        # dU = dh_raw * input.T -> (hidden_size ,1) * (vocab_size, 1).T -> (hidden_size, vocab_size)\n",
    "        dU += np.dot(dh_raw, inputs[t].T)\n",
    "        \n",
    "        # derivative of loss wrt V -> dh * dh_raw * dh_raw/dU\n",
    "        # dV = dh_raw * hidden_state.T -> (hidden_size ,1) * (hidden_size, 1).T -> (hidden_size, hidden_size)\n",
    "        dV += np.dot(dh_raw, hidden_states[t].T)\n",
    "        \n",
    "        # derivative of loss wrt dh_next\n",
    "        dh_next = np.dot(V.T, dh_raw)\n",
    "        \n",
    "        # derivative of loss wrt hb\n",
    "        dbh += dh_raw\n",
    "        \n",
    "    # for dparam in [dU, dV, dW, dbh, dby]:\n",
    "    #     np.clip(dparam, -1, 1, out=dparam) # clip to mitigate exploding gradients\n",
    "        \n",
    "    # put gradients into a tuple\n",
    "    gradients = (dU, dV, dW, dbh, dby)\n",
    "    \n",
    "    # gradient clipping to avoid exploding gradients\n",
    "    clip_gradients(gradients)\n",
    "    \n",
    "    # update parameters\n",
    "    updated_params = update_params(gradients, params, learning_rate)\n",
    "    \n",
    "    # return gradients and loss (for logging)\n",
    "    return updated_params, loss\n",
    "\n",
    "\n",
    "# function to update parameters according to the gradients\n",
    "def update_params(gradients, params, learning_rate):\n",
    "    for grad, p in zip(gradients, params):\n",
    "        p -= grad * learning_rate\n",
    "    return params\n",
    "\n",
    "# during training gradients may get too large which makes the model unstable, clipping function to avoid exploding gradients\n",
    "def clip_gradients(gradients, threshold=1):\n",
    "    for grad in gradients:\n",
    "        np.clip(grad, a_min=-threshold, a_max=threshold, out=grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below picture, taken from a blog post on Medium, makes a good illustration of the effect of gradient clipping. When the gradients are too large, the parameters take a huge descent and leave the good region -- causing instability to the model.\n",
    "\n",
    "There are different strategies to gradient clipping:\n",
    "- Gradient Norm Clipping - scaling the derivatives to have a given vector norm\n",
    "- **Value Clipping** - change the gradient greater/less than a threshold to that threshold. This is the simplest clipping method which we'll use in this example.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*vLFINWklJ0BtYtgzwK223g.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # to print progress bar\n",
    "\n",
    "\n",
    "def train(x, y, epochs, hidden_size, vocab_size, learning_rate=0.001, log_step=10):\n",
    "    # init random params\n",
    "    params = init_params(hidden_size, vocab_size)\n",
    "    \n",
    "    # save loss for plotting\n",
    "    loss_history = []\n",
    "    \n",
    "    for e in tqdm(range(epochs)):\n",
    "        # init epoch loss\n",
    "        epoch_loss = []\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            # forward propagation\n",
    "            outputs, hidden_states = forward_propagation(x[i], params, hidden_size)\n",
    "            \n",
    "            # back propagation and update parameters\n",
    "            params, loss = backward_propagation(x[i], y[i], outputs, hidden_states, params, learning_rate)\n",
    "\n",
    "            if np.isnan(loss):\n",
    "                raise ValueError('Gradients have vanished!')\n",
    "            \n",
    "            # save loss\n",
    "            epoch_loss.append(loss)\n",
    "            \n",
    "        # calculate the mean loss of the entire epoch    \n",
    "        loss_history.append(np.mean(epoch_loss))\n",
    "        \n",
    "        # print training results\n",
    "        if e % log_step ==0:\n",
    "            print(f\"Epoch {e}/{epochs} loss: {loss_history[-1]}\")\n",
    "            \n",
    "    # return trained parameters for using the model        \n",
    "    return params\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, trained_params, hidden_size, dictionary, top=5, ):  # predict single instance\n",
    "    # forward propagation on trained parameters\n",
    "    outputs, hidden_states = forward_propagation(x, trained_params,hidden_size)\n",
    "    \n",
    "    # extract the indices of the top5 probabilities of the output vector\n",
    "    vocab_ix = outputs[-1].flatten().argsort()[::-1][:top]\n",
    "    \n",
    "    # get the corresponding probabilities\n",
    "    prob = outputs[-1][vocab_ix]\n",
    "    \n",
    "    # map indices to the corresponding vocab\n",
    "    preds = [dictionary.get(i) for i in vocab_ix]\n",
    "    \n",
    "    # return the top predicted words and corresponding probabilities \n",
    "    return list(zip(preds, prob))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare fake data\n",
    "# the data we are generating is a sequence of letters, the first 2 letters are randomized, and the subsequent letters depend on the sum of the index of the preceeding letters \n",
    "# if the sum exceeds 25, i.e. the last index of all letters, it goes back to the beggining\n",
    "# e.g. if the index is 30, the corresponding letter would be at index = 30%26 = 4, i.e. 'e' \n",
    "# we can think of it as a variant of the Fibonacci Series ;)\n",
    "\n",
    "# generate data\n",
    "SEQUENCE_LEN = 5\n",
    "def generate_data(n_sample, sequence_len):\n",
    "    words = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    for i in range(n_sample):\n",
    "        arr = list(np.random.randint(26, size=2))\n",
    "        while len(arr) < sequence_len+1:\n",
    "            ix = sum(arr) if sum(arr) < 26 else sum(arr) % 26\n",
    "            arr.append(ix)\n",
    "        seq = [words[i] for i in arr]\n",
    "        inputs.append(seq[:-1])\n",
    "        outputs.append(seq[1:])\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "# function to encode the data\n",
    "def encode_data(inputs, outputs):\n",
    "    words = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "\n",
    "    word_ix = dict((c, i) for i, c in enumerate(words))\n",
    "\n",
    "    # initialize x,y as zeros\n",
    "    x = np.zeros(shape=(len(inputs), SEQUENCE_LEN, len(words), 1))\n",
    "    y = np.zeros(shape=(len(inputs), SEQUENCE_LEN, len(words), 1))\n",
    "\n",
    "    # loop to update x,y\n",
    "    for ix, sample in enumerate(inputs):\n",
    "        for t, word in enumerate(sample):\n",
    "            x[ix, t, word_ix[word]] = 1\n",
    "            y[ix, t, word_ix[outputs[ix][t]]] = 1\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "input_data, output_data = generate_data(n_sample=1000, sequence_len=SEQUENCE_LEN)\n",
    "x, y = encode_data(input_data, output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>[n, w, j, s, k]</td>\n",
       "      <td>[w, j, s, k, u]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>[s, t, l, w, s]</td>\n",
       "      <td>[t, l, w, s, k]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>[w, l, h, o, c]</td>\n",
       "      <td>[l, h, o, c, e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>[b, p, q, g, m]</td>\n",
       "      <td>[p, q, g, m, y]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>[w, w, s, k, u]</td>\n",
       "      <td>[w, s, k, u, o]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               input           output\n",
       "475  [n, w, j, s, k]  [w, j, s, k, u]\n",
       "524  [s, t, l, w, s]  [t, l, w, s, k]\n",
       "261  [w, l, h, o, c]  [l, h, o, c, e]\n",
       "606  [b, p, q, g, m]  [p, q, g, m, y]\n",
       "321  [w, w, s, k, u]  [w, s, k, u, o]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# let's put into a dataframe to better visualize the data\n",
    "df = pd.DataFrame({\n",
    "    'input': input_data,\n",
    "    'output': output_data\n",
    "})\n",
    "\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:00<00:45,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200 loss: 0.6266416735668658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:04<00:39,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 loss: 0.5744540348321556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [00:09<00:35,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 loss: 0.5653006691563859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [00:13<00:30,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200 loss: 0.5643238020421012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [00:18<00:26,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200 loss: 0.5634926486567031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [00:22<00:22,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200 loss: 0.5462572335381651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [00:26<00:17,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200 loss: 0.5306388204614744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [00:31<00:13,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200 loss: 0.5147042879402481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 161/200 [00:36<00:08,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200 loss: 0.5026572926760171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 181/200 [00:40<00:04,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200 loss: 0.49423312384303814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:44<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# define the hyperparameters\n",
    "epochs = 200\n",
    "hidden_size = 50\n",
    "vocab_size = 26\n",
    "learning_rate = 0.0001 # keeping the lr low is crucial to prevent gradient from blowing up and throw 'nan' loss\n",
    "log_step = epochs/10\n",
    "\n",
    "# train!\n",
    "params = train(x, y, epochs, hidden_size, vocab_size, learning_rate, log_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data:  ['r', 'r', 'i', 'q', 'g']\n",
      "predictions:  [('m', array([0.34766399])), ('g', array([0.12381405])), ('e', array([0.11245404])), ('i', array([0.06922135])), ('s', array([0.04318737]))]\n",
      "truth:  m\n"
     ]
    }
   ],
   "source": [
    "# test out the trained model\n",
    "\n",
    "# select a random sample from the data\n",
    "rand = np.random.randint(1000)\n",
    "print(\"input data: \",input_data[rand])\n",
    "\n",
    "# test input sample\n",
    "x_test = x[rand]\n",
    "\n",
    "# dictionary for mapping ix to word\n",
    "ix_word = dict((i, c) for i, c in enumerate(list(\"abcdefghijklmnopqrstuvwxyz\")))\n",
    "\n",
    "# predict\n",
    "predictions = predict(x_test, params, hidden_size=50, dictionary=ix_word, top=5)\n",
    "print('predictions: ', predictions)\n",
    "print('truth: ', output_data[rand][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! The model is able to pick up the pattern in just 200 training epochs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df3282b00461d0da48b5084f73affef31b9a54f8b85365a0cd8e11fce37c74e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
