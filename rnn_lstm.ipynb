{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla RNN suffers from the problem of vanishing gradients which causes the network to have short term memory and not learn properly. LSTM is variant of RNN and is designed specifically to tackle vanishing gradients.\n",
    "\n",
    "\n",
    "LSTM introduces 3 gates on top of vanilla RNN to control the flow of information in the sequence of data:\n",
    "- input modulation gate\n",
    "- input gate\n",
    "- forget gate: how much previous memory to forget\n",
    "- output gate\n",
    "\n",
    "These gates can be thought of as filters which control what goes in and what comes out.\n",
    "\n",
    "On a high level, the architecture of LSTM looks similar to vanilla RNN, where there are several recurrent unit, each producing an ouput, except that besides passing the hidden state from one unit to the next, each unit also passes along the cell state.\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190702161217/yt2.png\" width= 800/>\n",
    "\n",
    "\n",
    "The internal mechanism within an LSTM cell is illustrated below:\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190702161123/working3.png\" width= 800/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(hidden_size, vocab_size):\n",
    "    z_size = hidden_size + vocab_size # dimension of concatenated vector from prev hidden state and new input\n",
    "    \n",
    "    # weights matrix for input modulation, input, forget and output gates\n",
    "    Wg = np.random.randn(hidden_size, z_size) * 0.1\n",
    "    Wi = np.random.randn(hidden_size, z_size) * 0.1\n",
    "    Wf = np.random.randn(hidden_size, z_size) * 0.1\n",
    "    Wo = np.random.randn(hidden_size, z_size) * 0.1\n",
    "    \n",
    "    # weights for prediction - y_hat\n",
    "    Wy = np.random.randn(vocab_size, hidden_size) * 0.1\n",
    "    \n",
    "    # bias for input modulation, input, forget and output gates\n",
    "    bg = np.zeros((hidden_size, 1))\n",
    "    bi = np.zeros((hidden_size, 1))\n",
    "    bf = np.zeros((hidden_size, 1))\n",
    "    bo = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    # bias for prediction - y_hat\n",
    "    by = np.zeros(vocab_size, 1)\n",
    "    \n",
    "    return Wg, Wi, Wf, Wo, Wy, bg, bi, bf, bo, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(x, derivative=False): # squish the value between [0,1]\n",
    "    if not derivative:\n",
    "        return 1/(1+np.exp(-x))\n",
    "    else:\n",
    "        return sigmoid(x)*(1-sigmoid(x))\n",
    "    \n",
    "    \n",
    "def tanh(x, derivative=False): # squish the value between [-1,1]\n",
    "    if not derivative:\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    else:\n",
    "        return 1 - tanh(x)**2\n",
    "    \n",
    "    \n",
    "def softmax(x): # convert numbers into probabilities\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation \n",
    "def forward_propagation(x, hidden_size, vocab_size, params):\n",
    "    \n",
    "    # unpack params\n",
    "    Wg, Wi, Wf, Wo, Wy, bg, bi, bf, bo, by = params\n",
    "    \n",
    "    # store all hidden states & cell states for each time step\n",
    "    hidden_states, cell_states, outputs = [], [], []\n",
    "    \n",
    "    # initialize hidden state & cell state for first iteration\n",
    "    h_prev = np.zeros(hidden_size, 1)\n",
    "    c_prev = np.ones(hidden_size, 1) ##! init as ones, otherwise element-wise multiplication will zero out\n",
    "    \n",
    "    # iterate over the sequence\n",
    "    for input_vector in x:\n",
    "        # concat the hidden state and input vector\n",
    "        # shape -> (z_size, 1)\n",
    "        concat_vector = np.concatenate((h_prev, input_vector), axis=0)\n",
    "        \n",
    "        # input modulation gate\n",
    "        # shape -> (hidden_size, 1)\n",
    "        g = tanh(np.dot(Wg, concat_vector) + bg)\n",
    "        \n",
    "        # input gate\n",
    "        # shape -> (hidden_size, 1)\n",
    "        i = sigmoid(np.dot(Wi, concat_vector) + bi)\n",
    "        \n",
    "        # candidate memory - element-wise multiplication of g and i\n",
    "        # shape -> (hidden_size, 1)\n",
    "        candidate = g * i\n",
    "        \n",
    "        # forget gate\n",
    "        f = sigmoid(np.dot(Wf, concat_vector) + bf)\n",
    "        # element-wise multiplication with c_prev\n",
    "        c_curr = f * c_prev # intermediate cell state\n",
    "        \n",
    "        # update c_curr -> element-wise addition of c_curr & candidate memory\n",
    "        c_curr = c_curr + candidate\n",
    "        \n",
    "        # output gate\n",
    "        # shape -> (hidden_size, 1)\n",
    "        o = sigmoid(np.dot(Wo, concat_vector) + bo)\n",
    "        \n",
    "        # new hidden state - element-wise multiplication of cell state and output\n",
    "        # shape -> (hidden_size, 1)\n",
    "        h_curr = tanh(c_curr) * o\n",
    "        # update h_prev for next iteration\n",
    "        h_prev = h_curr\n",
    "        \n",
    "        # output - y_hat\n",
    "        # shape -> (vocab_size, 1)\n",
    "        y_hat = softmax(np.dot(Wy, o) + by)\n",
    "        outputs.append(y_hat)\n",
    "    \n",
    "            \n",
    "    return hidden_states, cell_states, outputs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_hat, y):\n",
    "    epsilon = 1e-12  # avoid log(0)\n",
    "    return -np.mean(np.log(y_hat + epsilon) * y)\n",
    "\n",
    "def backward_propagation(y, outputs, params):\n",
    "    # unpack parameters\n",
    "    Wg, Wi, Wf, Wo, Wy, bg, bi, bf, bo, by = params\n",
    "    \n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for t, y_hat in enumerate(reversed(outputs)):\n",
    "        loss += cross_entropy_loss(y_hat, y[t])\n",
    "        \n",
    "        # derivative of softmax with cross entropy\n",
    "        dy = outputs[t].copy()\n",
    "        dy = dy - y[t]\n",
    "        \n",
    "        # derivative of Wy\n",
    "        dWy\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df3282b00461d0da48b5084f73affef31b9a54f8b85365a0cd8e11fce37c74e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
