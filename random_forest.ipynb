{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of Random Forest is very simple once we get the hang of Decision Tree. The algorithm uses the **Bagging** method which draws **samples from the training data** as well as the **Feature Bagging** method that samples random **subsets of features** to build multiple decision trees, and finally make a final prediction based on the majority of the predictions of the decision trees, hence the name 'forest'.\n",
    "\n",
    "The idea behind is that we combine a number of weak learners to make better prediction overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, entropy=None, information_gain=None, value=None):\n",
    "        self.feature = feature  # index of the feature column\n",
    "        self.threshold = threshold  # threshold at which to split the data\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.entropy = entropy  # the entropy of the tree node before split\n",
    "        # the resultant information gain of the tree node after split\n",
    "        self.information_gain = information_gain\n",
    "        self.value = value  # leaf node value, if it's an intermediate tree node, value = None\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_sample=2, max_depth=5, criterion=\"entropy\"):\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "        self.root_node = None\n",
    "        self.criterion = criterion\n",
    "    \n",
    "    \n",
    "    # two cost functions, entropy & gini impurity to measure the purity of each tree node\n",
    "    @staticmethod\n",
    "    def entropy(array):\n",
    "        labels, counts = np.unique(array, return_counts=True)\n",
    "        count = {label: count for label, count in zip(labels, counts)}\n",
    "        entropy = 0\n",
    "        for i in count:\n",
    "            pct = count[i]/len(array)\n",
    "            entropy +=  -pct * np.log2(pct)\n",
    "        return entropy\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_impurity(array):\n",
    "        labels, counts = np.unique(array, return_counts=True)\n",
    "        count = {label: count for label, count in zip(labels, counts)}\n",
    "        n = len(array)\n",
    "        gini = 1 - sum([(count[i]/n)**2 for i in count])\n",
    "        return gini\n",
    "    \n",
    "    \n",
    "    # measure the information gain after split\n",
    "    def get_information_gain(self, parent, left_labels, right_labels):\n",
    "        n = len(parent)\n",
    "        if self.criterion == 'entropy':\n",
    "            children_entropy = self.entropy(left_labels)*len(left_labels)/n + self.entropy(right_labels)*len(right_labels)/n\n",
    "            gain = self.entropy(parent) - children_entropy\n",
    "            return gain\n",
    "        elif self.criterion == 'gini':\n",
    "            gain = self.gini_impurity(parent) - (self.gini_impurity(left_labels)*len(left_labels)/n + self.gini_impurity(right_labels)*len(right_labels)/n)\n",
    "            return gain\n",
    "    \n",
    "    \n",
    "    # helper method to split a data & labels into left & right given the feature & threshold\n",
    "    @staticmethod\n",
    "    def split(X, y, column_index, threshold):\n",
    "        left_indices, right_indices = [],[]\n",
    "        for ix, value in enumerate(X[:,column_index]):\n",
    "            if value < threshold:\n",
    "                left_indices.append(ix)\n",
    "            elif value >= threshold:\n",
    "                right_indices.append(ix)\n",
    "        return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n",
    "    \n",
    "    \n",
    "    # loop through each column and the values to evaluate all possible split and find the best split with the highest information gain\n",
    "    def find_best_split(self, X, y):\n",
    "        n_row, n_col = X.shape\n",
    "        best_information_gain = -999 \n",
    "        best_split_paramaters = {}\n",
    "        for col in range(n_col):\n",
    "            for row in range(n_row):\n",
    "                threshold = X[row, col]\n",
    "                left, right, left_labels, right_labels = self.split(X, y, column_index=col, threshold=threshold)\n",
    "                information_gain = self.get_information_gain(y, left_labels, right_labels) \n",
    "                if information_gain > best_information_gain:\n",
    "                    best_information_gain = information_gain\n",
    "                    best_split_paramaters = {'feature_index': col,\n",
    "                                             'threshold': threshold,\n",
    "                                             'left': left,\n",
    "                                             'left_labels': left_labels,\n",
    "                                             'right': right,\n",
    "                                             'right_labels': right_labels,\n",
    "                                             'information_gain': information_gain}\n",
    "                    \n",
    "        return best_split_paramaters\n",
    "    \n",
    "    \n",
    "    # return the final node\n",
    "    def leaf_node(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        most_common_label = np.argmax(counts)\n",
    "        return Node(value=most_common_label)\n",
    "    \n",
    "    \n",
    "    # recursive function to build the decision tree\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        n_row, n_col = X.shape\n",
    "        # conditions for recursively building the tree\n",
    "        if n_row > self.min_sample and depth < self.max_depth:\n",
    "            best_split_param = self.find_best_split(X, y)\n",
    "            if len(best_split_param['left']) == 0 or len(best_split_param['right']) == 0:\n",
    "                return self.leaf_node(y)\n",
    "            left_node = self.grow_tree(best_split_param['left'], best_split_param['left_labels'], depth=depth+1)\n",
    "            right_node = self.grow_tree(best_split_param['right'], best_split_param['right_labels'], depth=depth+1)\n",
    "            return Node(\n",
    "                feature=best_split_param['feature_index'], \n",
    "                threshold=best_split_param['threshold'], \n",
    "                left=left_node,\n",
    "                right=right_node,\n",
    "                entropy=self.entropy(y),\n",
    "                information_gain=best_split_param['information_gain']\n",
    "            )\n",
    "        # return leaf node, or terminal node, if conditions are not met\n",
    "        return self.leaf_node(y)\n",
    "    \n",
    "    \n",
    "    # build the decision tree based on input data\n",
    "    def fit(self, X, y):\n",
    "        self.root_node = self.grow_tree(X,y)\n",
    "        \n",
    "        \n",
    "    # predict method for single instance, i.e. 1D vector\n",
    "    def predict_single_instance(self, x, tree=None):\n",
    "        tree = self.root_node if tree == None else tree\n",
    "        if tree.value == None:\n",
    "            column_value = x[tree.feature]\n",
    "            if column_value >= tree.threshold:\n",
    "                return self.predict_single_instance(x, tree.right)\n",
    "            elif column_value < tree.threshold:\n",
    "                return self.predict_single_instance(x, tree.left)\n",
    "        return tree.value\n",
    "    \n",
    "    \n",
    "    # predict method for multiple instances, i.e. 2D array\n",
    "    def predict(self, X):\n",
    "        result = [self.predict_single_instance(row) for row in X]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=25, min_sample=2, max_depth=5):\n",
    "        self.n_trees = n_trees\n",
    "        self.min_sample = min_sample\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        \n",
    "    def get_sample(self, X, y):\n",
    "        n_row, n_col = X.shape\n",
    "        sample_index = np.random.choice(a=n_row, size=n_row, replace=True) # bagging - random sampling with replacement\n",
    "        rand_features = np.random.choice(a=n_col, size=np.random.randint(2, n_col), replace=False) # feature bagging - using random subsets of features\n",
    "        return X[sample_index, rand_features], y[sample_index, rand_features]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(self.trees) == 0:\n",
    "            self.trees = []\n",
    "        for tree in range(self.n_trees):\n",
    "            model = DecisionTree(\n",
    "                min_sample = self.min_sample,\n",
    "                max_depth = self.max_depth,\n",
    "            )\n",
    "            X_sample, y_sample = self.get_sample(X,y)\n",
    "            model.fit(X_sample, y_sample)\n",
    "            # save the model\n",
    "            self.trees.append(model)\n",
    "            \n",
    "    def predict_single_instance(self, x):\n",
    "        tree_predictions = [model.predict(x.reshape(1,-1)) for model in self.trees]\n",
    "        final_prediction = max(tree_predictions, key=tree_predictions.count)\n",
    "        return final_prediction\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self.predict_single_instance(x) for x in X]\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "X = data['data']\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# training\n",
    "model = RandomForest()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare with sklearn's Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9649122807017544"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy score\n",
    "accuracy_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though our implementation is much slower, we scored higher than Sklearn's Random Forest! Not bad!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "df3282b00461d0da48b5084f73affef31b9a54f8b85365a0cd8e11fce37c74e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
